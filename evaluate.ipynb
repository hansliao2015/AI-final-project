{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e10098e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "40ce6957",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"test\"\n",
    "gen_dir = os.path.join(base_dir, \"generated\")\n",
    "ref_dir = os.path.join(base_dir, \"ref\")\n",
    "text_dir = os.path.join(base_dir, \"text\")\n",
    "\n",
    "all_ids = sorted([\n",
    "    fname.split('.')[0]\n",
    "    for fname in os.listdir(gen_dir)\n",
    "    if fname.endswith('.png')\n",
    "])\n",
    "\n",
    "results = []\n",
    "\n",
    "for id_ in all_ids:\n",
    "    gen_path = os.path.join(gen_dir, f\"{id_}.png\")\n",
    "    ref_path = os.path.join(ref_dir, f\"{id_}.png\")\n",
    "    text_path = os.path.join(text_dir, f\"{id_}.txt\")\n",
    "\n",
    "    image_gen = Image.open(gen_path).convert(\"RGB\")\n",
    "    image_ref = Image.open(ref_path).convert(\"RGB\")\n",
    "\n",
    "    with open(text_path, 'r') as f:\n",
    "        text = f.read().strip()\n",
    "\n",
    "    inputs_gen = processor(text=[text], images=image_gen, return_tensors=\"pt\", padding=True)\n",
    "    inputs_ref = processor(text=[text], images=image_ref, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs_gen = model(**inputs_gen)\n",
    "        outputs_ref = model(**inputs_ref)\n",
    "\n",
    "    sim_gen = torch.cosine_similarity(outputs_gen.image_embeds, outputs_gen.text_embeds).item()\n",
    "    sim_ref = torch.cosine_similarity(outputs_ref.image_embeds, outputs_ref.text_embeds).item()\n",
    "\n",
    "    results.append((id_, sim_gen, sim_ref))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af922901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Similarity - Generated: 0.2305, Reference: 0.2297\n",
      "ID: 001 | Generated: 0.2789 | Reference: 0.2019\n",
      "ID: 002 | Generated: 0.1819 | Reference: 0.1859\n",
      "ID: 003 | Generated: 0.2240 | Reference: 0.2321\n",
      "ID: 004 | Generated: 0.1824 | Reference: 0.1743\n",
      "ID: 005 | Generated: 0.2069 | Reference: 0.2138\n",
      "ID: 006 | Generated: 0.2083 | Reference: 0.2039\n",
      "ID: 007 | Generated: 0.2316 | Reference: 0.2304\n",
      "ID: 008 | Generated: 0.2095 | Reference: 0.2138\n",
      "ID: 009 | Generated: 0.2136 | Reference: 0.2202\n",
      "ID: 010 | Generated: 0.2250 | Reference: 0.2195\n",
      "ID: 011 | Generated: 0.2647 | Reference: 0.2790\n",
      "ID: 012 | Generated: 0.2030 | Reference: 0.2138\n",
      "ID: 013 | Generated: 0.2034 | Reference: 0.2168\n",
      "ID: 014 | Generated: 0.2492 | Reference: 0.2546\n",
      "ID: 015 | Generated: 0.2699 | Reference: 0.2776\n",
      "ID: 016 | Generated: 0.2809 | Reference: 0.2768\n",
      "ID: 017 | Generated: 0.2856 | Reference: 0.2899\n",
      "Difference: 0.0770\n",
      "Difference: -0.0040\n",
      "Difference: -0.0081\n",
      "Difference: 0.0082\n",
      "Difference: -0.0069\n",
      "Difference: 0.0044\n",
      "Difference: 0.0012\n",
      "Difference: -0.0043\n",
      "Difference: -0.0065\n",
      "Difference: 0.0055\n",
      "Difference: -0.0142\n",
      "Difference: -0.0108\n",
      "Difference: -0.0135\n",
      "Difference: -0.0054\n",
      "Difference: -0.0078\n",
      "Difference: 0.0040\n",
      "Difference: -0.0044\n"
     ]
    }
   ],
   "source": [
    "generated_avg = sum(sim_gen for _, sim_gen, _ in results) / len(results)\n",
    "reference_avg = sum(sim_ref for _, _, sim_ref in results) / len(results)\n",
    "print(f\"Average Similarity - Generated: {generated_avg:.4f}, Reference: {reference_avg:.4f}\")\n",
    "\n",
    "for id_, sim_gen, sim_ref in results:\n",
    "    print(f\"ID: {id_} | Generated: {sim_gen:.4f} | Reference: {sim_ref:.4f}\")\n",
    "for id_, sim_gen, sim_ref in results:\n",
    "    print(f\"Difference: {(sim_gen - sim_ref):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b5c46720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始評估 30 組圖片的地理空間關係...\n",
      "已處理: 10/30\n",
      "已處理: 20/30\n",
      "已處理: 30/30\n",
      "=== 地理空間關係評估結果 ===\n",
      "總數量: 30\n",
      "生成圖片較高: 27 (90.0%)\n",
      "原始圖片較高: 3 (10.0%)\n",
      "平均分數 - 生成: 0.3544\n",
      "平均分數 - 原始: 0.2710\n",
      "平均分數差距: 0.0853\n",
      "詳細結果已保存至: geographic_evaluation_results.txt\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import re\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "class SimplifiedSpatialEvaluator:\n",
    "    def __init__(self, image_size=(512, 512)):\n",
    "        self.image_size = image_size\n",
    "        self.regions = self._define_spatial_regions()\n",
    "        \n",
    "    def _define_spatial_regions(self) -> Dict[str, Tuple[slice, slice]]:\n",
    "        h, w = self.image_size\n",
    "        return {\n",
    "            'north': (slice(0, h//3), slice(0, w)),\n",
    "            'south': (slice(2*h//3, h), slice(0, w)),\n",
    "            'east': (slice(0, h), slice(2*w//3, w)),\n",
    "            'west': (slice(0, h), slice(0, w//3)),\n",
    "            'center': (slice(h//3, 2*h//3), slice(w//3, 2*w//3)),\n",
    "            'northeast': (slice(0, h//2), slice(w//2, w)),\n",
    "            'northwest': (slice(0, h//2), slice(0, w//2)),\n",
    "            'southeast': (slice(h//2, h), slice(w//2, w)),\n",
    "            'southwest': (slice(h//2, h), slice(0, w//2)),\n",
    "            'northern': (slice(0, h//2), slice(0, w)),\n",
    "            'southern': (slice(h//2, h), slice(0, w)),\n",
    "            'eastern': (slice(0, h), slice(w//2, w)),\n",
    "            'western': (slice(0, h), slice(0, w//2)),\n",
    "            'entire_west': (slice(0, h), slice(0, w//2)),\n",
    "            'entire_east': (slice(0, h), slice(w//2, w)),\n",
    "            'entire_north': (slice(0, h//2), slice(0, w)),\n",
    "            'entire_south': (slice(h//2, h), slice(0, w)),\n",
    "        }\n",
    "\n",
    "class TerrainDetector:\n",
    "    @staticmethod\n",
    "    def detect_mountains(image_region: np.ndarray) -> float:\n",
    "        if len(image_region.shape) == 3:\n",
    "            gray = cv2.cvtColor(image_region, cv2.COLOR_RGB2GRAY)\n",
    "            hsv = cv2.cvtColor(image_region, cv2.COLOR_RGB2HSV)\n",
    "            rgb = image_region\n",
    "        else:\n",
    "            gray = image_region\n",
    "            hsv = None\n",
    "            rgb = None\n",
    "        white_peak_score = 0\n",
    "        if rgb is not None:\n",
    "            white_mask = ((rgb[:,:,0] >= 180) & (rgb[:,:,1] >= 180) & (rgb[:,:,2] >= 180))\n",
    "            light_gray_mask = ((rgb[:,:,0] >= 150) & (rgb[:,:,1] >= 150) & (rgb[:,:,2] >= 150) &\n",
    "                            (np.abs(rgb[:,:,0] - rgb[:,:,1]) <= 30) & \n",
    "                            (np.abs(rgb[:,:,1] - rgb[:,:,2]) <= 30))\n",
    "            \n",
    "            peak_mask = white_mask | light_gray_mask\n",
    "            white_peak_score = np.sum(peak_mask) / peak_mask.size\n",
    "            \n",
    "            if white_peak_score >= 0.15:\n",
    "                white_peak_score = min(white_peak_score * 2.0, 1.0)\n",
    "        \n",
    "        edges = cv2.Canny(gray, 30, 120)\n",
    "        edge_density = np.sum(edges > 0) / edges.size\n",
    "        \n",
    "        texture_score = np.std(gray) / 255.0\n",
    "\n",
    "        mountain_color_score = 0\n",
    "        if hsv is not None:\n",
    "            # 棕色山脈\n",
    "            brown_mask = ((hsv[:,:,0] >= 5) & (hsv[:,:,0] <= 25) & \n",
    "                        (hsv[:,:,1] >= 30) & (hsv[:,:,2] >= 40))\n",
    "            # 灰色山脈\n",
    "            gray_mask = ((hsv[:,:,1] <= 60) & (hsv[:,:,2] >= 60) & (hsv[:,:,2] <= 220))\n",
    "            # 綠褐色山脈\n",
    "            green_brown_mask = ((hsv[:,:,0] >= 20) & (hsv[:,:,0] <= 60) & \n",
    "                            (hsv[:,:,1] >= 20) & (hsv[:,:,2] >= 50))\n",
    "            \n",
    "            mountain_color_score = (np.sum(brown_mask) + np.sum(gray_mask) + \n",
    "                                np.sum(green_brown_mask)) / (hsv.shape[0] * hsv.shape[1])\n",
    "        \n",
    "        # 5. 高度變化檢測\n",
    "        laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "        height_variation = np.std(laplacian) / 255.0\n",
    "        \n",
    "        # 6. 山峰形狀檢測\n",
    "        peak_shape_score = 0\n",
    "        if rgb is not None:\n",
    "            # 使用形態學操作檢測山峰形狀\n",
    "            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "            peaks = cv2.morphologyEx(peak_mask.astype(np.uint8), cv2.MORPH_OPEN, kernel)\n",
    "            \n",
    "            # 檢測三角形或尖峰狀結構\n",
    "            contours, _ = cv2.findContours(peaks, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            for contour in contours:\n",
    "                if cv2.contourArea(contour) > 100:  # 足夠大的區域\n",
    "                    # 計算凸包\n",
    "                    hull = cv2.convexHull(contour)\n",
    "                    hull_area = cv2.contourArea(hull)\n",
    "                    contour_area = cv2.contourArea(contour)\n",
    "                    if hull_area > 0:\n",
    "                        convexity = contour_area / hull_area\n",
    "                        if 0.6 <= convexity <= 0.95:  # 接近三角形\n",
    "                            peak_shape_score = max(peak_shape_score, convexity)\n",
    "        \n",
    "        # 綜合評分 - 重點強化白色山峰檢測\n",
    "        mountain_score = (\n",
    "            white_peak_score * 0.35 +      # 白色山峰是關鍵特徵\n",
    "            edge_density * 0.2 +\n",
    "            texture_score * 0.15 +\n",
    "            mountain_color_score * 0.15 +\n",
    "            height_variation * 0.1 +\n",
    "            peak_shape_score * 0.05\n",
    "        )\n",
    "        \n",
    "        return min(mountain_score, 1.0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_seas(image_region: np.ndarray) -> float:\n",
    "        \"\"\"檢測海洋特徵\"\"\"\n",
    "        if len(image_region.shape) == 3:\n",
    "            hsv = cv2.cvtColor(image_region, cv2.COLOR_RGB2HSV)\n",
    "            gray = cv2.cvtColor(image_region, cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            gray = image_region\n",
    "            hsv = None\n",
    "            \n",
    "        # 1. 藍色檢測\n",
    "        if hsv is not None:\n",
    "            blue_mask1 = ((hsv[:,:,0] >= 90) & (hsv[:,:,0] <= 130) & \n",
    "                         (hsv[:,:,1] >= 50) & (hsv[:,:,2] >= 50))\n",
    "            blue_mask2 = ((hsv[:,:,0] >= 100) & (hsv[:,:,0] <= 120) & \n",
    "                         (hsv[:,:,1] >= 100) & (hsv[:,:,2] >= 30))\n",
    "            blue_ratio = (np.sum(blue_mask1) + np.sum(blue_mask2)) / (hsv.shape[0] * hsv.shape[1])\n",
    "        else:\n",
    "            blue_ratio = np.sum(gray <= 120) / gray.size\n",
    "            \n",
    "        # 2. 平滑度\n",
    "        smoothness = 1.0 - (np.std(gray) / 255.0)\n",
    "        \n",
    "        # 3. 大面積連通性\n",
    "        if hsv is not None:\n",
    "            water_mask = blue_mask1 | blue_mask2\n",
    "        else:\n",
    "            water_mask = gray <= 120\n",
    "            \n",
    "        labeled, num_features = ndimage.label(water_mask)\n",
    "        if num_features > 0:\n",
    "            sizes = ndimage.sum(water_mask, labeled, range(num_features + 1))\n",
    "            max_size = np.max(sizes[1:]) if len(sizes) > 1 else 0\n",
    "            connectivity_score = max_size / water_mask.size\n",
    "        else:\n",
    "            connectivity_score = 0\n",
    "            \n",
    "        # 4. 邊緣特徵\n",
    "        edges = cv2.Canny(gray, 30, 100)\n",
    "        edge_simplicity = 1.0 - (np.sum(edges > 0) / edges.size)\n",
    "        \n",
    "        sea_score = (\n",
    "            blue_ratio * 0.4 +\n",
    "            smoothness * 0.2 +\n",
    "            connectivity_score * 0.3 +\n",
    "            edge_simplicity * 0.1\n",
    "        )\n",
    "        \n",
    "        return min(sea_score, 1.0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_lakes(image_region: np.ndarray) -> float:\n",
    "        \"\"\"檢測湖泊特徵\"\"\"\n",
    "        if len(image_region.shape) == 3:\n",
    "            hsv = cv2.cvtColor(image_region, cv2.COLOR_RGB2HSV)\n",
    "            gray = cv2.cvtColor(image_region, cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            gray = image_region\n",
    "            hsv = None\n",
    "            \n",
    "        # 1. 藍色檢測\n",
    "        if hsv is not None:\n",
    "            blue_mask = ((hsv[:,:,0] >= 90) & (hsv[:,:,0] <= 130) & \n",
    "                        (hsv[:,:,1] >= 30) & (hsv[:,:,2] >= 40))\n",
    "            blue_ratio = np.sum(blue_mask) / (hsv.shape[0] * hsv.shape[1])\n",
    "        else:\n",
    "            blue_ratio = np.sum(gray <= 130) / gray.size\n",
    "            \n",
    "        # 2. 中等大小的連通區域\n",
    "        if hsv is not None:\n",
    "            water_mask = blue_mask\n",
    "        else:\n",
    "            water_mask = gray <= 130\n",
    "            \n",
    "        labeled, num_features = ndimage.label(water_mask)\n",
    "        medium_size_score = 0\n",
    "        if num_features > 0:\n",
    "            sizes = ndimage.sum(water_mask, labeled, range(num_features + 1))\n",
    "            if len(sizes) > 1:\n",
    "                valid_sizes = sizes[1:]\n",
    "                total_area = water_mask.size\n",
    "                for size in valid_sizes:\n",
    "                    ratio = size / total_area\n",
    "                    if 0.05 <= ratio <= 0.6:\n",
    "                        medium_size_score = max(medium_size_score, ratio)\n",
    "            \n",
    "        # 3. 平滑度\n",
    "        smoothness = 1.0 - (np.std(gray) / 255.0)\n",
    "        \n",
    "        # 4. 形狀規則性\n",
    "        shape_score = 0\n",
    "        if np.sum(water_mask) > 100:\n",
    "            contours, _ = cv2.findContours(water_mask.astype(np.uint8), \n",
    "                                         cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            if contours:\n",
    "                largest_contour = max(contours, key=cv2.contourArea)\n",
    "                area = cv2.contourArea(largest_contour)\n",
    "                perimeter = cv2.arcLength(largest_contour, True)\n",
    "                if perimeter > 0:\n",
    "                    circularity = 4 * np.pi * area / (perimeter * perimeter)\n",
    "                    shape_score = min(circularity, 1.0)\n",
    "            \n",
    "        lake_score = (\n",
    "            blue_ratio * 0.3 +\n",
    "            medium_size_score * 0.4 +\n",
    "            smoothness * 0.2 +\n",
    "            shape_score * 0.1\n",
    "        )\n",
    "        \n",
    "        return min(lake_score, 1.0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_rivers(image_region: np.ndarray) -> float:\n",
    "        \"\"\"檢測河流特徵\"\"\"\n",
    "        if len(image_region.shape) == 3:\n",
    "            hsv = cv2.cvtColor(image_region, cv2.COLOR_RGB2HSV)\n",
    "            gray = cv2.cvtColor(image_region, cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            gray = image_region\n",
    "            hsv = None\n",
    "            \n",
    "        # 1. 藍色檢測\n",
    "        if hsv is not None:\n",
    "            blue_mask = ((hsv[:,:,0] >= 90) & (hsv[:,:,0] <= 130) & \n",
    "                        (hsv[:,:,1] >= 30) & (hsv[:,:,2] >= 30))\n",
    "            blue_ratio = np.sum(blue_mask) / (hsv.shape[0] * hsv.shape[1])\n",
    "        else:\n",
    "            blue_ratio = np.sum(gray <= 120) / gray.size\n",
    "            \n",
    "        # 2. 細長形狀檢測\n",
    "        if hsv is not None:\n",
    "            water_mask = blue_mask\n",
    "        else:\n",
    "            water_mask = gray <= 120\n",
    "            \n",
    "        # 形態學操作\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "        cleaned_mask = cv2.morphologyEx(water_mask.astype(np.uint8), \n",
    "                                      cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        # 骨架化檢測線性結構\n",
    "        try:\n",
    "            skeleton = cv2.ximgproc.thinning(cleaned_mask)\n",
    "            skeleton_ratio = np.sum(skeleton > 0) / max(np.sum(cleaned_mask > 0), 1)\n",
    "        except:\n",
    "            # 如果沒有ximgproc，使用替代方法\n",
    "            skeleton_ratio = 0.5\n",
    "        \n",
    "        # 3. 長寬比檢測\n",
    "        contours, _ = cv2.findContours(cleaned_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        elongation_score = 0\n",
    "        if contours:\n",
    "            for contour in contours:\n",
    "                if cv2.contourArea(contour) > 50:\n",
    "                    rect = cv2.minAreaRect(contour)\n",
    "                    width, height = rect[1]\n",
    "                    if min(width, height) > 0:\n",
    "                        aspect_ratio = max(width, height) / min(width, height)\n",
    "                        if aspect_ratio > 3:\n",
    "                            elongation_score = max(elongation_score, \n",
    "                                                 min(aspect_ratio / 10, 1.0))\n",
    "        \n",
    "        # 4. 連續性檢測\n",
    "        labeled, num_features = ndimage.label(water_mask)\n",
    "        continuity_score = 1.0 / max(num_features, 1) if num_features > 0 else 0\n",
    "            \n",
    "        river_score = (\n",
    "            blue_ratio * 0.2 +\n",
    "            skeleton_ratio * 0.3 +\n",
    "            elongation_score * 0.3 +\n",
    "            continuity_score * 0.2\n",
    "        )\n",
    "        \n",
    "        return min(river_score, 1.0)\n",
    "\n",
    "class TextParser:\n",
    "    def __init__(self):\n",
    "        self.direction_patterns = {\n",
    "            'north': r'\\b(north|northern|top|upper)\\b',\n",
    "            'south': r'\\b(south|southern|bottom|lower)\\b', \n",
    "            'east': r'\\b(east|eastern|right)\\b',\n",
    "            'west': r'\\b(west|western|left)\\b',\n",
    "            'center': r'\\b(center|central|middle)\\b',\n",
    "            'northeast': r'\\b(northeast|north-east|upper right)\\b',\n",
    "            'northwest': r'\\b(northwest|north-west|upper left)\\b',\n",
    "            'southeast': r'\\b(southeast|south-east|lower right)\\b',\n",
    "            'southwest': r'\\b(southwest|south-west|lower left)\\b',\n",
    "            'entire_west': r'\\b(entire west|whole west|throughout west)\\b',\n",
    "            'entire_east': r'\\b(entire east|whole east|throughout east)\\b',\n",
    "            'entire_north': r'\\b(entire north|whole north|throughout north)\\b',\n",
    "            'entire_south': r'\\b(entire south|whole south|throughout south)\\b',\n",
    "        }\n",
    "        \n",
    "        self.terrain_patterns = {\n",
    "            'mountains': r'\\b(mountain|mountains|mountainous|peak|peaks|hill|hills|highland|range)\\b',\n",
    "            'seas': r'\\b(sea|seas|ocean|oceans)\\b',\n",
    "            'lakes': r'\\b(lake|lakes|pond|ponds)\\b',\n",
    "            'rivers': r'\\b(river|rivers|stream|streams|waterway|waterways)\\b',\n",
    "        }\n",
    "    \n",
    "    def parse_description(self, text: str) -> List[Dict[str, Any]]:\n",
    "        text = text.lower()\n",
    "        spatial_relations = []\n",
    "        \n",
    "        # 1. 先解析複合描述\n",
    "        compound_relations = self._parse_compound_descriptions(text)\n",
    "        spatial_relations.extend(compound_relations)\n",
    "        \n",
    "        # 2. 解析單獨描述，按句子分割避免錯誤匹配\n",
    "        individual_relations = self._parse_individual_descriptions(text)\n",
    "        \n",
    "        # 3. 合併結果，避免重複\n",
    "        for rel in individual_relations:\n",
    "            if not self._is_duplicate_relation(rel, spatial_relations):\n",
    "                spatial_relations.append(rel)\n",
    "        \n",
    "        return spatial_relations\n",
    "    \n",
    "    def _parse_compound_descriptions(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"解析複合描述，如 'center and east are all lake'\"\"\"\n",
    "        relations = []\n",
    "        \n",
    "        # 模式1: \"A and B are all X\" 或 \"A and B are X\"\n",
    "        patterns = [\n",
    "            r'(\\w+)\\s+and\\s+(\\w+)\\s+are\\s+all\\s+(\\w+)',\n",
    "            r'(\\w+)\\s+and\\s+(\\w+)\\s+are\\s+(\\w+)',\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            matches = re.finditer(pattern, text)\n",
    "            for match in matches:\n",
    "                dir1, dir2, terrain = match.groups()\n",
    "                \n",
    "                if (self._is_valid_direction(dir1) and \n",
    "                    self._is_valid_direction(dir2) and \n",
    "                    self._is_valid_terrain(terrain)):\n",
    "                    \n",
    "                    relations.append({\n",
    "                        'terrain': self._normalize_terrain(terrain),\n",
    "                        'direction': dir1,\n",
    "                        'coverage': 'extensive' if 'all' in match.group(0) else 'partial',\n",
    "                        'confidence': 0.9,\n",
    "                        'context': match.group(0)\n",
    "                    })\n",
    "                    relations.append({\n",
    "                        'terrain': self._normalize_terrain(terrain),\n",
    "                        'direction': dir2,\n",
    "                        'coverage': 'extensive' if 'all' in match.group(0) else 'partial',\n",
    "                        'confidence': 0.9,\n",
    "                        'context': match.group(0)\n",
    "                    })\n",
    "        \n",
    "        # 模式2: \"X in the A and B\"\n",
    "        pattern2 = r'(\\w+)\\s+in\\s+the\\s+(\\w+)\\s+and\\s+(\\w+)'\n",
    "        matches = re.finditer(pattern2, text)\n",
    "        for match in matches:\n",
    "            terrain, dir1, dir2 = match.groups()\n",
    "            \n",
    "            if (self._is_valid_terrain(terrain) and\n",
    "                self._is_valid_direction(dir1) and \n",
    "                self._is_valid_direction(dir2)):\n",
    "                \n",
    "                relations.append({\n",
    "                    'terrain': self._normalize_terrain(terrain),\n",
    "                    'direction': dir1,\n",
    "                    'coverage': 'partial',\n",
    "                    'confidence': 0.8,\n",
    "                    'context': match.group(0)\n",
    "                })\n",
    "                relations.append({\n",
    "                    'terrain': self._normalize_terrain(terrain),\n",
    "                    'direction': dir2,\n",
    "                    'coverage': 'partial',\n",
    "                    'confidence': 0.8,\n",
    "                    'context': match.group(0)\n",
    "                })\n",
    "        \n",
    "        return relations\n",
    "    \n",
    "    def _parse_individual_descriptions(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"解析單獨的地形描述 - 按句子分割\"\"\"\n",
    "        spatial_relations = []\n",
    "        \n",
    "        # 按標點符號分割句子，避免跨句匹配\n",
    "        sentences = re.split(r'[,;]|\\s+while\\s+|\\s+and\\s+(?=the)', text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "            \n",
    "            # 在每個句子中尋找地形和對應的方向\n",
    "            sentence_relations = self._parse_single_sentence(sentence)\n",
    "            spatial_relations.extend(sentence_relations)\n",
    "        \n",
    "        return spatial_relations\n",
    "    \n",
    "    def _parse_single_sentence(self, sentence: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"解析單個句子中的地形描述\"\"\"\n",
    "        relations = []\n",
    "        \n",
    "        for terrain, terrain_pattern in self.terrain_patterns.items():\n",
    "            terrain_matches = re.finditer(terrain_pattern, sentence)\n",
    "            \n",
    "            for terrain_match in terrain_matches:\n",
    "                # 在同一句子中尋找方向\n",
    "                direction_found = False\n",
    "                \n",
    "                for direction, direction_pattern in self.direction_patterns.items():\n",
    "                    if re.search(direction_pattern, sentence):\n",
    "                        # 確定覆蓋程度\n",
    "                        coverage = 'partial'\n",
    "                        if re.search(r'\\b(all|entire|whole|cover|covering)\\b', sentence):\n",
    "                            coverage = 'extensive'\n",
    "                        \n",
    "                        relations.append({\n",
    "                            'terrain': terrain,\n",
    "                            'direction': direction,\n",
    "                            'coverage': coverage,\n",
    "                            'confidence': self._calculate_confidence(sentence, terrain, direction),\n",
    "                            'context': sentence\n",
    "                        })\n",
    "                        direction_found = True\n",
    "                        break\n",
    "                \n",
    "                # 如果沒有找到明確方向，標記為一般性描述\n",
    "                if not direction_found:\n",
    "                    relations.append({\n",
    "                        'terrain': terrain,\n",
    "                        'direction': 'general',\n",
    "                        'coverage': 'partial',\n",
    "                        'confidence': 0.3,\n",
    "                        'context': sentence\n",
    "                    })\n",
    "        \n",
    "        return relations\n",
    "    \n",
    "    def _is_valid_direction(self, direction: str) -> bool:\n",
    "        \"\"\"檢查是否是有效的方向詞\"\"\"\n",
    "        direction_words = ['north', 'south', 'east', 'west', 'center', 'central', 'middle',\n",
    "                          'northeast', 'northwest', 'southeast', 'southwest',\n",
    "                          'northern', 'southern', 'eastern', 'western']\n",
    "        return direction in direction_words\n",
    "    \n",
    "    def _is_valid_terrain(self, terrain: str) -> bool:\n",
    "        \"\"\"檢查是否是有效的地形詞\"\"\"\n",
    "        terrain_words = ['mountain', 'mountains', 'mountainous', 'lake', 'lakes', \n",
    "                        'sea', 'seas', 'ocean', 'oceans', 'river', 'rivers',\n",
    "                        'peak', 'peaks', 'hill', 'hills', 'pond', 'ponds']\n",
    "        return terrain in terrain_words\n",
    "    \n",
    "    def _normalize_terrain(self, terrain: str) -> str:\n",
    "        \"\"\"標準化地形名稱\"\"\"\n",
    "        if terrain in ['mountain', 'mountains', 'mountainous', 'peak', 'peaks', 'hill', 'hills']:\n",
    "            return 'mountains'\n",
    "        elif terrain in ['lake', 'lakes', 'pond', 'ponds']:\n",
    "            return 'lakes'\n",
    "        elif terrain in ['sea', 'seas', 'ocean', 'oceans']:\n",
    "            return 'seas'\n",
    "        elif terrain in ['river', 'rivers']:\n",
    "            return 'rivers'\n",
    "        return terrain\n",
    "    \n",
    "    def _is_duplicate_relation(self, new_rel: Dict, existing_rels: List[Dict]) -> bool:\n",
    "        \"\"\"檢查是否是重複的關係\"\"\"\n",
    "        for existing in existing_rels:\n",
    "            if (existing['terrain'] == new_rel['terrain'] and \n",
    "                existing['direction'] == new_rel['direction']):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def _calculate_confidence(self, context: str, terrain: str, direction: str) -> float:\n",
    "        \"\"\"計算關係的可信度\"\"\"\n",
    "        confidence = 0.6  # 基礎信心度\n",
    "        \n",
    "        # 距離因子\n",
    "        terrain_pos = context.find(terrain)\n",
    "        direction_pos = context.find(direction.replace('_', ' '))\n",
    "        if terrain_pos != -1 and direction_pos != -1:\n",
    "            distance = abs(terrain_pos - direction_pos)\n",
    "            confidence += max(0, (20 - distance) / 20 * 0.2)\n",
    "        \n",
    "        # 關鍵詞加分\n",
    "        if 'all' in context:\n",
    "            confidence += 0.15\n",
    "        if 'cover' in context or 'covering' in context:\n",
    "            confidence += 0.1\n",
    "        if 'are' in context:\n",
    "            confidence += 0.05\n",
    "            \n",
    "        return min(confidence, 1.0)\n",
    "\n",
    "class GeographicEvaluator:\n",
    "    \"\"\"地理特徵評估器\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.spatial_evaluator = SimplifiedSpatialEvaluator()\n",
    "        self.terrain_detector = TerrainDetector()\n",
    "        self.text_parser = TextParser()\n",
    "    \n",
    "    def evaluate_geographic_accuracy(self, image: np.ndarray, description: str) -> Dict[str, Any]:\n",
    "        \"\"\"評估地理描述的準確性\"\"\"\n",
    "        \n",
    "        # 調整圖像大小\n",
    "        if image.shape[:2] != self.spatial_evaluator.image_size:\n",
    "            image = cv2.resize(image, self.spatial_evaluator.image_size)\n",
    "        \n",
    "        # 解析文字描述\n",
    "        spatial_relations = self.text_parser.parse_description(description)\n",
    "        \n",
    "        # 評估每個空間關係\n",
    "        relation_scores = []\n",
    "        detailed_results = {}\n",
    "        \n",
    "        for relation in spatial_relations:\n",
    "            terrain = relation['terrain']\n",
    "            direction = relation['direction']\n",
    "            coverage = relation['coverage']\n",
    "            \n",
    "            if direction == 'general':\n",
    "                terrain_score = self._evaluate_terrain_globally(image, terrain)\n",
    "            else:\n",
    "                if direction in self.spatial_evaluator.regions:\n",
    "                    region_slice = self.spatial_evaluator.regions[direction]\n",
    "                    region = image[region_slice]\n",
    "                    \n",
    "                    if terrain == 'mountains':\n",
    "                        terrain_score = self.terrain_detector.detect_mountains(region)\n",
    "                    elif terrain == 'seas':\n",
    "                        terrain_score = self.terrain_detector.detect_seas(region)\n",
    "                    elif terrain == 'lakes':\n",
    "                        terrain_score = self.terrain_detector.detect_lakes(region)\n",
    "                    elif terrain == 'rivers':\n",
    "                        terrain_score = self.terrain_detector.detect_rivers(region)\n",
    "                    else:\n",
    "                        terrain_score = 0.0\n",
    "                else:\n",
    "                    terrain_score = 0.0\n",
    "            \n",
    "            if coverage == 'extensive' and direction != 'general':\n",
    "                coverage_score = self._evaluate_coverage(region, terrain)\n",
    "                final_score = terrain_score * coverage_score\n",
    "            else:\n",
    "                final_score = terrain_score\n",
    "            \n",
    "            relation_score = final_score * relation['confidence']\n",
    "            relation_scores.append(relation_score)\n",
    "            \n",
    "            detailed_results[f\"{terrain}_{direction}\"] = {\n",
    "                'terrain_score': terrain_score,\n",
    "                'coverage_score': coverage_score if coverage == 'extensive' else 1.0,\n",
    "                'confidence': relation['confidence'],\n",
    "                'final_score': relation_score,\n",
    "                'context': relation['context']\n",
    "            }\n",
    "        \n",
    "        overall_score = np.mean(relation_scores) if relation_scores else 0.0\n",
    "        \n",
    "        return {\n",
    "            'overall_score': overall_score,\n",
    "            'individual_relations': detailed_results,\n",
    "            'parsed_relations': spatial_relations,\n",
    "            'total_relations_found': len(spatial_relations)\n",
    "        }\n",
    "    \n",
    "    def _evaluate_terrain_globally(self, image: np.ndarray, terrain: str) -> float:\n",
    "        \"\"\"全局評估地形特徵\"\"\"\n",
    "        if terrain == 'mountains':\n",
    "            return self.terrain_detector.detect_mountains(image)\n",
    "        elif terrain == 'seas':\n",
    "            return self.terrain_detector.detect_seas(image)\n",
    "        elif terrain == 'lakes':\n",
    "            return self.terrain_detector.detect_lakes(image)\n",
    "        elif terrain == 'rivers':\n",
    "            return self.terrain_detector.detect_rivers(image)\n",
    "        return 0.0\n",
    "    \n",
    "    def _evaluate_coverage(self, region: np.ndarray, terrain: str) -> float:\n",
    "        \"\"\"評估地形覆蓋程度\"\"\"\n",
    "        h, w = region.shape[:2]\n",
    "        grid_size = 32\n",
    "        coverage_count = 0\n",
    "        total_grids = 0\n",
    "        \n",
    "        for i in range(0, h, grid_size):\n",
    "            for j in range(0, w, grid_size):\n",
    "                grid = region[i:i+grid_size, j:j+grid_size]\n",
    "                if grid.size > 0:\n",
    "                    if terrain == 'mountains':\n",
    "                        score = self.terrain_detector.detect_mountains(grid)\n",
    "                    elif terrain == 'seas':\n",
    "                        score = self.terrain_detector.detect_seas(grid)\n",
    "                    elif terrain == 'lakes':\n",
    "                        score = self.terrain_detector.detect_lakes(grid)\n",
    "                    elif terrain == 'rivers':\n",
    "                        score = self.terrain_detector.detect_rivers(grid)\n",
    "                    else:\n",
    "                        score = 0.0\n",
    "                    \n",
    "                    if score > 0.3:\n",
    "                        coverage_count += 1\n",
    "                    total_grids += 1\n",
    "        \n",
    "        return coverage_count / total_grids if total_grids > 0 else 0.0\n",
    "\n",
    "# 適配您的檔案結構的主要評估函數\n",
    "def evaluate_geographic_spatial_relations():\n",
    "    \"\"\"評估地理空間關係 - 適配您的檔案結構\"\"\"\n",
    "    \n",
    "    # 初始化評估器\n",
    "    evaluator = GeographicEvaluator()\n",
    "    \n",
    "    # 設定目錄路徑\n",
    "    base_dir = \"test\"\n",
    "    gen_dir = os.path.join(base_dir, \"generated\")\n",
    "    ref_dir = os.path.join(base_dir, \"ref\")\n",
    "    text_dir = os.path.join(base_dir, \"text\")\n",
    "    \n",
    "    # 獲取所有ID\n",
    "    all_ids = sorted([\n",
    "        fname.split('.')[0]\n",
    "        for fname in os.listdir(gen_dir)\n",
    "        if fname.endswith('.png')\n",
    "    ])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"開始評估 {len(all_ids)} 組圖片的地理空間關係...\")\n",
    "    \n",
    "    for i, id_ in enumerate(all_ids):\n",
    "        gen_path = os.path.join(gen_dir, f\"{id_}.png\")\n",
    "        ref_path = os.path.join(ref_dir, f\"{id_}.png\")\n",
    "        text_path = os.path.join(text_dir, f\"{id_}.txt\")\n",
    "        \n",
    "        try:\n",
    "            # 載入圖片\n",
    "            image_gen = Image.open(gen_path).convert(\"RGB\")\n",
    "            image_ref = Image.open(ref_path).convert(\"RGB\")\n",
    "            \n",
    "            # 轉換為numpy陣列\n",
    "            image_gen_np = np.array(image_gen)\n",
    "            image_ref_np = np.array(image_ref)\n",
    "            \n",
    "            # 讀取文字描述\n",
    "            with open(text_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read().strip()\n",
    "            \n",
    "            # 評估生成圖片的地理準確性\n",
    "            result_gen = evaluator.evaluate_geographic_accuracy(image_gen_np, text)\n",
    "            \n",
    "            # 評估參考圖片的地理準確性\n",
    "            result_ref = evaluator.evaluate_geographic_accuracy(image_ref_np, text)\n",
    "            \n",
    "            # 計算分數\n",
    "            score_gen = result_gen['overall_score']\n",
    "            score_ref = result_ref['overall_score']\n",
    "            \n",
    "            # 決定勝出者\n",
    "            winner = 'generated' if score_gen > score_ref else 'reference'\n",
    "            score_diff = abs(score_gen - score_ref)\n",
    "            \n",
    "            results.append({\n",
    "                'id': id_,\n",
    "                'score_gen': score_gen,\n",
    "                'score_ref': score_ref,\n",
    "                'winner': winner,\n",
    "                'score_difference': score_diff,\n",
    "                'text': text,\n",
    "                'details_gen': result_gen,\n",
    "                'details_ref': result_ref\n",
    "            })\n",
    "            \n",
    "            # 進度顯示\n",
    "            if (i + 1) % 10 == 0 or i == len(all_ids) - 1:\n",
    "                print(f\"已處理: {i + 1}/{len(all_ids)}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"處理 {id_} 時發生錯誤: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_results(results):\n",
    "    \"\"\"分析評估結果\"\"\"\n",
    "    if not results:\n",
    "        print(\"沒有結果可分析\")\n",
    "        return\n",
    "    \n",
    "    # 統計資訊\n",
    "    total_count = len(results)\n",
    "    gen_wins = sum(1 for r in results if r['winner'] == 'generated')\n",
    "    ref_wins = sum(1 for r in results if r['winner'] == 'reference')\n",
    "    \n",
    "    avg_score_gen = np.mean([r['score_gen'] for r in results])\n",
    "    avg_score_ref = np.mean([r['score_ref'] for r in results])\n",
    "    avg_score_diff = np.mean([r['score_difference'] for r in results])\n",
    "    \n",
    "    print(\"=== 地理空間關係評估結果 ===\")\n",
    "    print(f\"總數量: {total_count}\")\n",
    "    print(f\"生成圖片較高: {gen_wins} ({gen_wins/total_count*100:.1f}%)\")\n",
    "    print(f\"原始圖片較高: {ref_wins} ({ref_wins/total_count*100:.1f}%)\")\n",
    "    print(f\"平均分數 - 生成: {avg_score_gen:.4f}\")\n",
    "    print(f\"平均分數 - 原始: {avg_score_ref:.4f}\")\n",
    "    print(f\"平均分數差距: {avg_score_diff:.4f}\")\n",
    "    \n",
    "def save_detailed_results(results, output_file='geographic_evaluation_results.txt'):\n",
    "    \"\"\"保存詳細結果到檔案\"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"地理空間關係評估詳細結果\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        for result in results:\n",
    "            f.write(f\"ID: {result['id']}\\n\")\n",
    "            f.write(f\"描述: {result['text']}\\n\")\n",
    "            f.write(f\"生成圖片分數: {result['score_gen']:.4f}\\n\")\n",
    "            f.write(f\"原始圖片分數: {result['score_ref']:.4f}\\n\")\n",
    "            f.write(f\"較高者: {result['winner']}\\n\")\n",
    "            f.write(f\"分數差距: {result['score_difference']:.4f}\\n\")\n",
    "            f.write(\"\\n生成圖片分析:\\n\")\n",
    "            for relation, details in result['details_gen']['individual_relations'].items():\n",
    "                f.write(f\"  {relation}: {details['final_score']:.4f}\\n\")\n",
    "            \n",
    "            f.write(\"\\n原始圖片分析:\\n\")\n",
    "            for relation, details in result['details_ref']['individual_relations'].items():\n",
    "                f.write(f\"  {relation}: {details['final_score']:.4f}\\n\")\n",
    "            f.write(\"\\n\" + \"-\" * 50 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"詳細結果已保存至: {output_file}\")\n",
    "\n",
    "# 主執行函數\n",
    "def main():\n",
    "    results = evaluate_geographic_spatial_relations()\n",
    "    analyze_results(results)\n",
    "    save_detailed_results(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
